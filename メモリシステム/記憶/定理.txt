ローカル LLM のためのコラッツ予想メモリシステム
============================================================================================

この文書では、いくつかの数学的な予想や定理を組み合わせて、ローカル言語モデルのコンテキストメモリを管理する方法を簡潔に説明します。目的は、会話の追跡性を失うことなく、コンテキストウィンドウのオーバーフローを避けることです。

1. **レーベンシュタイン距離 – セマンティックメトリック**

   * モデルの出力は固定サイズの断片に分割されます。
   * 各断片について類似度（レーベンシュタイン距離）が推定されます。 **最もユニークな**断片と**最も共通する**断片が選択されます。
   * 結果は 2 つの一時ファイルに保存されます。`unique_fragment.txt` には最も繰り返しが少ない断片が、`common_fragment.txt` には最も頻繁な断片が保存されます。

2. **コラッツ予想 – 拡張と縮小のダイナミクス**

   * セッションの各要素（ユーザー入力、アシスタントの応答、選択されたユニークおよび共通の断片）ごとに 1 を加算して *論理長* が計算されます。
   * 合計が**偶数**の場合、メモリは**拡張**されます: 会話の最後の 4 つのやり取りがメモリファイルに保持されます。
   * 合計が**奇数**の場合、メモリは**縮小**されます: メモリファイルの半分（古い半分）が削除され、最も最近のやり取りのみが保存されます。

3. **ゴールドバッハ予想 – メモリ構造**

   * コラッツステップを適用した後、メモリファイルの行数を数えます。
   * この合計となる 2 つの素数を探します。内容はこの素数分割に従って 2 つの部分に分割され、大きい方の素数に対応する断片が保持されます。これにより、メモリが常に素数長の 2 つの断片に分割されることが保証されます。

4. **リーマン予想（運用版） – 回復**

   * これらの操作後にメモリファイルが 1 行に縮小された場合は、前のバックアップを使用して前のメッセージの半分を復元し、コンテキストを補充します。概念的にはそれでも単一の認知単位と見なされますが、突然の情報損失を防ぐためにより多くのコンテキストが提供されます。

これらのルールにより、チャットメモリは**時間的に知的**になります: 最近の会話の重要な部分を保持し、コンテキストに寄与しなくなったものを制御された方法で忘れ、極端な損失が発生した場合には、対話が一貫性を保つために必要最低限の情報を回復します。
