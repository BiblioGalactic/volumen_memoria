# 本地 LLM 的 Collatz 猜想记忆系统

本软件包演示了如何应用数学启发式方法来管理本地语言模型的上下文记忆（通过 [llama.cpp](https://github.com/ggerganov/llama.cpp)）。该系统在保留相关对话的同时防止上下文窗口溢出。

## 包含的文件

| 文件 / 目录                | 描述 |
|---------------------------|------|
| `启动_记忆系统.sh`        | 一个 Bash 脚本，用于启动具有内存控制和自动清理功能的本地 LLM。 |
| `记忆/定理.txt`           | 结合莱文斯坦距离、Collatz 和 Goldbach 猜想以及里曼假设的操作版的启发式方法的完整说明。 |
| `记忆/提示.txt`           | 定义助手名称和角色的初始提示。 |
| `记忆/记忆.txt`           | 用于存储动态对话记忆的文件。每次运行时自动更新。 |
| `日志/`                   | 写入执行日志的目录。主要日志文件是 `记忆系统.log`。 |

## 要求

* 具备 **bash** 和标准工具 `split`、`wc`、`tail` 和 `head` 的类 Unix 环境。
* 从 `llama.cpp` 编译的 `llama-cli` 二进制文件和兼容的模型文件。默认情况下脚本期望：
  - `$HOME/modelo/llama.cpp/build/bin/llama-cli` 下的 `llama-cli`
  - `$HOME/modelo/modelos_grandes/M6/mistral-7b-instruct-v0.1.Q6_K.gguf` 下的模型文件

  如果你的路径不同，请在 `启动_记忆系统.sh` 顶部调整 `LLAMA_CLI` 和 `MODEL_FILE` 变量。

## 使用方法

1. 将本包中的所有文件和目录放在同一文件夹中。
2. 使脚本具有可执行权限：

   ```bash
   chmod +x 启动_记忆系统.sh
   ```

3. 运行脚本：

   ```bash
   ./启动_记忆系统.sh
   ```

   每次调用期间，脚本将：

   * 验证所需的文件是否存在，以及外部命令 `llama-cli`、`split`、`wc`、`tail` 和 `head` 是否可用。
   * 使用简化的莱文斯坦距离从记忆文件中选择语义片段（最长且最独特的片段）。
   * 组合基础提示和选定的片段构建临时提示。
   * 使用固定标志调用 `llama-cli` 以生成响应。
   * 将最后的用户输入和助手响应附加到 `记忆/记忆.txt`。
   * 应用在 `记忆/定理.txt` 中描述的数学启发式（Collatz、Goldbach 和 Riemann 步骤）来扩展或收缩记忆。
   * 将进度消息和时间戳写入 `日志/记忆系统.log`。
   * 完成后清理临时文件。

4. 查看 `日志/记忆系统.log` 以查看执行记录，查看 `记忆/记忆.txt` 以了解在多次运行中保留了哪些对话片段。

## 注意事项

* `levenshtein_fragment_selector` 函数使用简化的方法，并 **不会** 计算真正的莱文斯坦距离。用于生产时，请考虑实现基于真实距离的比较器。
* 你可以通过修改脚本中的 `TOKENS_MAX` 和 `FRAGMENT_SIZE` 变量来调整最大令牌限制和片段大小。
* 此系统旨在作为一个 **结构和功能原型**，并不打算替代生产环境中专业的记忆管理技术。
